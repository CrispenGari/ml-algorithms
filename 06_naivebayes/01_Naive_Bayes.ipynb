{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_Naive_Bayes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3F6e3sXwbeu"
      },
      "source": [
        "### Naive Bayes Classifier\n",
        "\n",
        "\n",
        "A Naive Bayes classifier is a probabilistic machine learning model that’s used for classification task. \n",
        "\n",
        "* Naive Bayes algorithim is a supervised machine learning algorithim based on the **Bayes theorem** mainly used for text classification.\n",
        "\n",
        "### Why is it called Naive Bayes?\n",
        "\n",
        "1. **Naïve:** It is called Naïve because it assumes that the occurrence of a certain feature is independent of the occurrence of other features. Such as if the fruit is identified on the bases of color, shape, and taste, then red, spherical, and sweet fruit is recognized as an apple. Hence each feature individually contributes to identify that it is an apple without depending on each other.\n",
        "\n",
        "2. **Bayes:** It is called Bayes because it depends on the principle of Bayes' Theorem.\n",
        "\n",
        "### Bayes Theorem\n",
        "\n",
        "With the Bayes theorem we can find the probability of $A$ given that $B$ has occurred as $B$ is the evidence and $A$ is the hypothesis.\n",
        "\n",
        "The assumption made here is that the predictors/features are independent, the presence of one particular feature does not affect the other.\n",
        "\n",
        "The formula for Bayes'theorem is given as:\n",
        "\n",
        "![img](https://static.javatpoint.com/tutorial/machine-learning/images/naive-bayes-classifier-algorithm.png)\n",
        "\n",
        "Where:\n",
        "\n",
        "1. **P(A|B) is Posterior probability:** Probability of hypothesis $A$ on the observed event $B$.\n",
        "\n",
        "2. **P(B|A) is Likelihood probability**: Probability of the evidence given that the probability of a hypothesis is true.\n",
        "\n",
        "3. **P(A) is Prior Probability:** Probability of hypothesis before observing the evidence.\n",
        "\n",
        "4. **P(B) is Marginal Probability:** Probability of Evidence.\n",
        "\n",
        "\n",
        "\n",
        "According to this example, Bayes theorem can be rewritten as:\n",
        "\n",
        "![img](https://miro.medium.com/max/484/1*Gb2Ifjn1olE5ML6mqY5WNQ.png)\n",
        "\n",
        "Where $X$ is given as \n",
        "\n",
        "```\n",
        "X = (x1, x2, x3,...,xn)\n",
        "```\n",
        "\n",
        "x1 upto xn represents the features and by substituting X and exapanding using the chain rule we will get:\n",
        "\n",
        "![img](https://miro.medium.com/max/700/1*y1pZM0oYjQfMJt0rDX-REA.png)\n",
        "\n",
        "\n",
        "In our case, the class variable(y) has only two outcomes, yes or no. There could be cases where the classification could be multivariate. Therefore, we need to find the class y with maximum probability.\n",
        "\n",
        "![img](https://miro.medium.com/max/700/1*0AWiuPgpHElcqB2X-fpVkw.png)\n",
        "\n",
        "### The formular for conditional probability changes to\n",
        "\n",
        "![img](https://miro.medium.com/max/700/1*0If5Mey7FnW_RktMM5BkaQ.png)\n",
        "\n",
        "### Implementation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSr0Vl22vxHb"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBNOaGezwati"
      },
      "source": [
        "class NaiveBayes:\n",
        "  def __init__(self):\n",
        "    self._classes = None\n",
        "    self._mean = None\n",
        "    self._var = None\n",
        "    self._priors = None\n",
        "\n",
        "  # the fit method\n",
        "  def fit(self, X, y):\n",
        "    n_sample, n_features = X.shape\n",
        "    self._classes = np.unique(y)\n",
        "    n_classes = len(self._classes)\n",
        "\n",
        "    # calculating the var, mean and prior for each class\n",
        "    self._mean = np.zeros((n_classes, n_features), dtype=np.float32)\n",
        "    self._var = np.zeros((n_classes, n_features), dtype=np.float32)\n",
        "    self._priors = np.zeros(n_classes, dtype=np.float32)\n",
        "\n",
        "    for i, c in enumerate(self._classes):\n",
        "      X_c = X[y == c]\n",
        "      self._mean[i, :] = X_c.mean(axis=0)\n",
        "      self._var[i, :] = X_c.var(axis=0)\n",
        "      self._priors[i] = X_c.shape[0]/float(n_sample)\n",
        "  \n",
        "\n",
        "   # the predict method\n",
        "  def predict(self, X):\n",
        "    #  predict label for each feature\n",
        "    return np.array([self._predict(x) for x in X])\n",
        "\n",
        "  def _predict(self, x):\n",
        "    posteriors = list()\n",
        "    # calculate the posterior probability for each class\n",
        "\n",
        "    for i, c in enumerate(self._classes):\n",
        "      prior = np.log(self._priors[i]) \n",
        "      posterior = np.sum(np.log(self._pdf(i, x)))\n",
        "      posterior = prior + posterior\n",
        "      posteriors.append(posterior)\n",
        "    # return the class with the highest probability\n",
        "    return self._classes[np.argmax(np.array(posteriors))]\n",
        "\n",
        "  def _pdf(self, class_idx, x):\n",
        "    mean = self._mean[class_idx]\n",
        "    var = self._var[class_idx]\n",
        "    numerator = np.exp(-((x - mean) ** 2) / (2 * var))\n",
        "    denominator = np.sqrt(2 * np.pi * var)\n",
        "    return numerator/denominator\n",
        "\n",
        "  # the evaluate function\n",
        "  def evaluate(self, y_true, y_pred):\n",
        "    print(\"accuracy: \",  np.sum(y_true == y_pred) / len(y_true))\n",
        "    "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWSsbmpo5GUp"
      },
      "source": [
        "### Testing our Naive Bayes Classifier\n",
        "\n",
        "We are going to create a toy dataset using the `make_classsification` dataset from `sklearn` and evaluate our machine learning model afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uT8VRkL3waqz"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nEewtlU5hCt"
      },
      "source": [
        "Creating the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNP3hlVAwaoS"
      },
      "source": [
        "X, y = datasets.make_classification(\n",
        "        n_samples=10000, n_features=10, n_classes=2, random_state=42\n",
        ")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=123\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJZjbkQn5p8h"
      },
      "source": [
        "Classifier instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vMvGh04walu"
      },
      "source": [
        "clf = NaiveBayes()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUl9NB0Lwai4"
      },
      "source": [
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao66NMjY5-GU"
      },
      "source": [
        "Evaluating the classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abU92sv_wagD",
        "outputId": "0e4e81de-a2ef-4212-cdc4-3f413df6495c"
      },
      "source": [
        "y_preds = clf.predict(X_test)\n",
        "clf.evaluate(y_test, y_preds)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:  0.8745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHh1deB36ps4"
      },
      "source": [
        "### Refs\n",
        "1. [www.geeksforgeeks.org](https://www.geeksforgeeks.org/naive-bayes-classifiers/)\n",
        "2. [towardsdatascience.com](https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c)\n",
        "3. [www.javatpoint.com](https://www.javatpoint.com/machine-learning-naive-bayes-classifier)"
      ]
    }
  ]
}