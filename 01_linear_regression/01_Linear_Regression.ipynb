{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_Linear_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccMBd8sol8_d"
      },
      "source": [
        "### Linear Regression\n",
        "\n",
        "In this notebook we are going to implement linear regression machine leaning algorithim from scratch using the `numpy` package.\n",
        "\n",
        "In machine leaning Linear Regression is used to model or predict continuos variables. Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. In Linear regression we are trying to predict a number.\n",
        "\n",
        "\n",
        "The formular of a linear regression line is given by:\n",
        "\n",
        "$\\hat{y}$ = **_mx_ + _b_**\n",
        "\n",
        "\n",
        "### The cost function\n",
        "\n",
        "The cost function (or loss function) is used to measure the performance of a machine learning model or quantifies the error between the expected values and the values predicted by our hypothetical function. The cost function for Linear Regression is represented by $J$. Our goal is to try to reduce the value of $J$.\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "<img with=\"100%\" src=\"https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-ee804dd2ef914445d34e803be76167a2_l3.svg\" alt=\"\" align=\"center\" />\n",
        "</p>\n",
        "\n",
        "> Here, ``m`` is the total number of training examples in the dataset.\n",
        "``y(i)`` represents the value of target variable for ith training example.\n",
        "\n",
        "\n",
        "### Gradient descent: \n",
        "\n",
        "We start with a certain value of `w` which is our weighs and we keep on updating the value of `w` until we get to a minimum value.\n",
        "\n",
        "\n",
        "### Updating the weights and bias.\n",
        "\n",
        "To update the weights `w` and bias `b` we do it as follows in every iteration.\n",
        "\n",
        "1. $w$ = $w$ - $\\alpha$ * $dw$\n",
        "\n",
        "2. $b$ = $b$ - $\\alpha$ * $db$\n",
        "\n",
        "Where:\n",
        "* $\\alpha$ - is the learning rate\n",
        "* $w$ - is the weights\n",
        "* $b$ - is the bias\n",
        "\n",
        "\n",
        "If `dw` and `db` are simply the derivative of the loss function with regards to the weights and biases. Given the loss function\n",
        "\n",
        "$J = \\frac{1}{m} \\Sigma_{i=1}^{m}(y_i - h(x_i))^2$\n",
        "\n",
        "The derivatives of the loss to the weights (`dw`) and bias (`db`) are equal to:\n",
        "\n",
        "\n",
        "1. `dw`\n",
        "\n",
        "$\\frac{\\partial}{\\partial W} J = -\\frac{2}{m} \\Sigma_{i=1}^{m}(y_i - h(x_i)) * x_i$\n",
        "\n",
        "\n",
        "2. `db`\n",
        "\n",
        "\n",
        "$\\frac{\\partial}{\\partial b} J = -\\frac{2}{m} \\Sigma_{i=1}^{m}(y_i - h(x_i))$\n",
        "\n",
        "\n",
        "### Implementation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOq8SL4Fl80b"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUrzpfT5xJ_P"
      },
      "source": [
        "\n",
        "In regression we don't evaluate our model based on `accuracy` we can use `mse` , `r2score` or `mae`.\n",
        "\n",
        "\n",
        "### MSE (Mean Squared Error)\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://www.gstatic.com/education/formulas2/397133473/en/mean_squared_error.svg\"/>\n",
        "</p>\n",
        "\n",
        "* $n$ = number of samples\n",
        "* $y$ = observed values\n",
        "* $\\hat{y}$ = predicted values\n",
        "\n",
        "### MAE (Mean Absolute Error)\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://www.gstatic.com/education/formulas2/397133473/en/mean_absolute_error.svg\"/>\n",
        "</p>\n",
        "\n",
        "* $n$ = number of samples\n",
        "* $y$ = observed values\n",
        "* $\\hat{y}$ = predicted values\n",
        "\n",
        "### r2 score\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://www.gstatic.com/education/formulas2/397133473/en/coefficient_of_determination.svg\"/>\n",
        "</p>\n",
        "\n",
        "* $R^2$\t=\tcoefficient of determination\n",
        "* $RSS$\t=\tsum of squares of residuals\n",
        "* $TSS$\t=\ttotal sum of squares\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMQvyntLl8xe"
      },
      "source": [
        "class LinearRegression:\n",
        "  def __init__(self, lr=0.001, n_iters=10000):\n",
        "    self.lr = lr\n",
        "    self.n_iters = n_iters\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    n_samples, n_features = X.shape\n",
        "\n",
        "    \"\"\"\n",
        "    you can initialize with random numbers\n",
        "    \"\"\"\n",
        "    self.w = np.zeros(n_features)\n",
        "    self.b = 0\n",
        "\n",
        "    # gradient descent\n",
        "    for _ in range(self.n_iters):\n",
        "      y_predicted = np.dot(X, self.w) + self.b\n",
        "      # y= mx + b\n",
        "\n",
        "      # compute dw and db (gradients)\n",
        "      dw = (2 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "      db = (2 / n_samples) * np.sum(y_predicted -y)\n",
        "\n",
        "      # updating the parameters\n",
        "      self.w -= self.lr * dw\n",
        "      self.b -= self.lr * db\n",
        "\n",
        "  def predict(self, X):\n",
        "      # y = mx + b\n",
        "      return np.dot(X, self.w) + self.b\n",
        "\n",
        "  \"\"\"\n",
        "  In the evaluate method we want to calculate:\n",
        "  1. mse\n",
        "  2. mae\n",
        "  \"\"\"\n",
        "\n",
        "  def _mse(self, y_true, y_pred):\n",
        "      return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "  def _mae(self, y_true, y_pred):\n",
        "      return np.mean(np.abs((y_true - y_pred)))\n",
        "\n",
        "  def _r2_score(self, y_true, y_pred):\n",
        "    corr_matrix = np.corrcoef(y_true, y_pred)\n",
        "    corr = corr_matrix[0, 1]\n",
        "    return corr ** 2\n",
        "\n",
        "  def evaluate(self, y_true, y_pred):\n",
        "      print(\"mae: \", self._mae(y_true, y_pred))\n",
        "      print(\"mse: \", self._mse(y_true, y_pred))\n",
        "      print(\"r2-score: \", self._r2_score(y_true, y_pred))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFKYJ0MJ0g7B"
      },
      "source": [
        "### Testing the regressor\n",
        "\n",
        "We are going to get the data from `sklearn` library as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vudJpDe9l8uq"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV8pydyFl8rv"
      },
      "source": [
        "X, y = datasets.make_regression(\n",
        "        n_samples=100, n_features=2, noise=20, random_state=4\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxi0-_Ck085n"
      },
      "source": [
        "Spitting the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHuTpvPCl8pC"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFVNbIvXl8md"
      },
      "source": [
        "regressor = LinearRegression(lr=0.01)\n",
        "regressor.fit(X_train, y_train)\n",
        "y_preds = regressor.predict(X_test)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XahQ0JAl8j2",
        "outputId": "72a80306-a275-4909-ad4f-5547e3e575d3"
      },
      "source": [
        "y_preds[:2], y_test[:2]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([62.33532614, 75.11878242]), array([51.87312664, 53.15882384]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e6Z5UVRl8g9",
        "outputId": "14c96e67-df6e-445f-8b89-64ed7c2bc490"
      },
      "source": [
        "regressor.evaluate(y_test, y_preds)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mae:  21.0945482217351\n",
            "mse:  675.9536315181933\n",
            "r2-score:  0.8362375703177033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCFqWms5l3qC"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}