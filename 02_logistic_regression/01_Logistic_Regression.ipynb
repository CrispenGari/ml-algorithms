{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_Logistic_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccMBd8sol8_d"
      },
      "source": [
        "### Logistic Regression\n",
        "\n",
        "In this notebook we are going to implement logistic regression machine leaning algorithim from scratch using the `numpy` package.\n",
        "\n",
        "In machine leaning Logistic Regression is used to model the probability of a certain class or event existing such as pass/fail, win/lose, etc.\n",
        "\n",
        "\n",
        "The formular of a logistic regression is the same as the formular for linear regression is just that the predictions are applied the `sigmoid` function to them:\n",
        "\n",
        "$\\hat{y}$ = **_mx_ + _b_**\n",
        "\n",
        "### Sigmoid function\n",
        "\n",
        "The `sigmoid` function always returns a value between `0` and `1`.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://www.gstatic.com/education/formulas2/397133473/en/sigmoid_function.svg\" alt=\"\"/>\n",
        "</p>\n",
        "\n",
        "### Gradient descent: \n",
        "\n",
        "We start with a certain value of `w` which is our weighs and we keep on updating the value of `w` until we get to a minimum value.\n",
        "\n",
        "\n",
        "### Updating the weights and bias.\n",
        "\n",
        "To update the weights `w` and bias `b` we do it as follows in every iteration.\n",
        "\n",
        "1. $w$ = $w$ - $\\alpha$ * $dw$\n",
        "\n",
        "2. $b$ = $b$ - $\\alpha$ * $db$\n",
        "\n",
        "Where:\n",
        "* $\\alpha$ - is the learning rate\n",
        "* $w$ - is the weights\n",
        "* $b$ - is the bias\n",
        "\n",
        "\n",
        "If `dw` and `db` are simply the derivative of the loss function with regards to the weights and biases. Given the loss function\n",
        "\n",
        "$J = \\frac{1}{m} \\Sigma_{i=1}^{m}(y_i - h(x_i))^2$\n",
        "\n",
        "The derivatives of the loss to the weights (`dw`) and bias (`db`) are equal to:\n",
        "\n",
        "\n",
        "1. `dw`\n",
        "\n",
        "$\\frac{\\partial}{\\partial W} J = -\\frac{2}{m} \\Sigma_{i=1}^{m}(y_i - h(x_i)) * x_i$\n",
        "\n",
        "\n",
        "2. `db`\n",
        "\n",
        "\n",
        "$\\frac{\\partial}{\\partial b} J = -\\frac{2}{m} \\Sigma_{i=1}^{m}(y_i - h(x_i))$\n",
        "\n",
        "\n",
        "### Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOq8SL4Fl80b"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUrzpfT5xJ_P"
      },
      "source": [
        "\n",
        "In logistic regression unlike in the linear regression we can be able to calculate the accuracy between observed labels and predicted labels. We can use classification metrics to measure the performance of the algorithm such as confusion matrix, accuracy, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMQvyntLl8xe"
      },
      "source": [
        "class LogisticRegression:\n",
        "  def __init__(self, lr=0.001, n_iters=10000):\n",
        "    self.lr = lr\n",
        "    self.n_iters = n_iters\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    n_samples, n_features = X.shape\n",
        "\n",
        "    \"\"\"\n",
        "    you can initialize with random numbers\n",
        "    \"\"\"\n",
        "    self.w = np.zeros(n_features)\n",
        "    self.b = 0\n",
        "\n",
        "    # gradient descent\n",
        "    for _ in range(self.n_iters):\n",
        "      linear_model = np.dot(X, self.w) + self.b\n",
        "      y_predicted = self._sigmoid(linear_model)\n",
        "      # y= mx + b\n",
        "\n",
        "      # compute dw and db (gradients)\n",
        "      dw = (2 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "      db = (2 / n_samples) * np.sum(y_predicted -y)\n",
        "\n",
        "      # updating the parameters\n",
        "      self.w -= self.lr * dw\n",
        "      self.b -= self.lr * db\n",
        "\n",
        "  def predict(self, X):\n",
        "      linear_model = np.dot(X, self.w) + self.b\n",
        "      y_predicted = self._sigmoid(linear_model)\n",
        "      preds = [0 if i< 0.5 else 1 for i in y_predicted]\n",
        "      return np.array(preds)\n",
        "\n",
        "  def _sigmoid(self, x):\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "  def _accuracy(self, y_true, y_pred ):\n",
        "    return np.sum(y_true == y_pred) / len(y_true)\n",
        "\n",
        "  def evaluate(self, y_true, y_pred):\n",
        "      print(\"acc: \", self._accuracy(y_true, y_pred))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFKYJ0MJ0g7B"
      },
      "source": [
        "### Testing the classifier\n",
        "\n",
        "We are going to get the data from `sklearn` library as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vudJpDe9l8uq"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV8pydyFl8rv"
      },
      "source": [
        "bc = datasets.load_breast_cancer()\n",
        "X, y = bc.data, bc.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=1234\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxi0-_Ck085n"
      },
      "source": [
        "Spitting the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHuTpvPCl8pC"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFVNbIvXl8md",
        "outputId": "ab1c23ce-7892-4473-9b34-ca1a61f59c34"
      },
      "source": [
        "regressor = LogisticRegression(lr=0.001)\n",
        "regressor.fit(X_train, y_train)\n",
        "y_preds = regressor.predict(X_test)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: RuntimeWarning: overflow encountered in exp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XahQ0JAl8j2",
        "outputId": "c186900a-fae0-482b-bd65-fdbc1b5142ef"
      },
      "source": [
        "y_preds[:10], y_test[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 0, 0, 1, 1, 0, 0, 0, 1, 1]), array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e6Z5UVRl8g9",
        "outputId": "f52edac4-33d7-4b05-810a-315909c4411c"
      },
      "source": [
        "regressor.evaluate(y_test, y_preds)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc:  0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCFqWms5l3qC"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}